{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec: How To Implement word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Pre-trained Embeddings\n",
    "\n",
    "Some other options:\n",
    "- `glove-twitter-{25/50/100/200}`\n",
    "- `glove-wiki-gigaword-{50/200/300}`\n",
    "- `word2vec-google-news-300`\n",
    "- `word2vec-ruscorpora-news-300`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\yauheni_leaniuk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.0.1)WARNING: You are using pip version 21.0.1; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\yauheni_leaniuk\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\yauheni_leaniuk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim) (1.20.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\yauheni_leaniuk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\yauheni_leaniuk\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gensim) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "# Install gensim\n",
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained word vectors using gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "wiki_embeddings = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-0.32307 , -0.87616 ,  0.21977 ,  0.25268 ,  0.22976 ,  0.7388  ,\n",
       "       -0.37954 , -0.35307 , -0.84369 , -1.1113  , -0.30266 ,  0.33178 ,\n",
       "       -0.25113 ,  0.30448 , -0.077491, -0.89815 ,  0.092496, -1.1407  ,\n",
       "       -0.58324 ,  0.66869 , -0.23122 , -0.95855 ,  0.28262 , -0.078848,\n",
       "        0.75315 ,  0.26584 ,  0.3422  , -0.33949 ,  0.95608 ,  0.065641,\n",
       "        0.45747 ,  0.39835 ,  0.57965 ,  0.39267 , -0.21851 ,  0.58795 ,\n",
       "       -0.55999 ,  0.63368 , -0.043983, -0.68731 , -0.37841 ,  0.38026 ,\n",
       "        0.61641 , -0.88269 , -0.12346 , -0.37928 , -0.38318 ,  0.23868 ,\n",
       "        0.6685  , -0.43321 , -0.11065 ,  0.081723,  1.1569  ,  0.78958 ,\n",
       "       -0.21223 , -2.3211  , -0.67806 ,  0.44561 ,  0.65707 ,  0.1045  ,\n",
       "        0.46217 ,  0.19912 ,  0.25802 ,  0.057194,  0.53443 , -0.43133 ,\n",
       "       -0.34311 ,  0.59789 , -0.58417 ,  0.068995,  0.23944 , -0.85181 ,\n",
       "        0.30379 , -0.34177 , -0.25746 , -0.031101, -0.16285 ,  0.45169 ,\n",
       "       -0.91627 ,  0.64521 ,  0.73281 , -0.22752 ,  0.30226 ,  0.044801,\n",
       "       -0.83741 ,  0.55006 , -0.52506 , -1.7357  ,  0.4751  , -0.70487 ,\n",
       "        0.056939, -0.7132  ,  0.089623,  0.41394 , -1.3363  , -0.61915 ,\n",
       "       -0.33089 , -0.52881 ,  0.16483 , -0.98878 ], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Explore the word vector for \"king\"\n",
    "wiki_embeddings['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('prince', 0.7682328820228577),\n",
       " ('queen', 0.7507690787315369),\n",
       " ('son', 0.7020888328552246),\n",
       " ('brother', 0.6985775232315063),\n",
       " ('monarch', 0.6977890729904175),\n",
       " ('throne', 0.6919989585876465),\n",
       " ('kingdom', 0.6811409592628479),\n",
       " ('father', 0.6802029013633728),\n",
       " ('emperor', 0.6712858080863953),\n",
       " ('ii', 0.6676074266433716)]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Find the words most similar to king based on the trained word vectors\n",
    "wiki_embeddings.most_similar('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Our Own Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Read in the data and clean up column names\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "messages = pd.read_csv('C:\\\\Users\\\\Yauheni_Leaniuk\\\\Documents\\\\Python\\\\Data_Engineer\\\\Advanced NLP Python for ML\\\\data\\\\spam.csv', encoding='latin-1')\n",
    "messages = messages.drop(labels = [\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
    "messages.columns = [\"label\", \"text\"]\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                            text_clean  \n",
       "0  [go, until, jurong, point, crazy, available, only, in, bugis, great, world, la, buffet, cine, th...  \n",
       "1                                                                          [ok, lar, joking, wif, oni]  \n",
       "2  [free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive,...  \n",
       "3                                                       [dun, say, so, early, hor, already, then, say]  \n",
       "4                                [nah, don, think, he, goes, to, usf, he, lives, around, here, though]  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>text</th>\n      <th>text_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n      <td>[go, until, jurong, point, crazy, available, only, in, bugis, great, world, la, buffet, cine, th...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>[ok, lar, joking, wif, oni]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n      <td>[free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>[dun, say, so, early, hor, already, then, say]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n      <td>[nah, don, think, he, goes, to, usf, he, lives, around, here, though]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Clean data using the built in cleaner in gensim\n",
    "messages['text_clean'] = messages['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(messages['text_clean'],\n",
    "                                                    messages['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the word2vec model\n",
    "w2v_model = gensim.models.Word2Vec(X_train, vector_size=100, window=5, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-0.03030046,  0.06656196,  0.0132574 , -0.00940217,  0.00982651,\n",
       "       -0.09994359,  0.03243558,  0.12566341, -0.04995817, -0.05821013,\n",
       "       -0.03057883, -0.09388538, -0.0024758 ,  0.02312848,  0.01997338,\n",
       "       -0.04218255,  0.02290968, -0.05198505, -0.01050435, -0.10790614,\n",
       "        0.03519291,  0.02683081,  0.03065212, -0.0358837 ,  0.00951856,\n",
       "        0.02329191, -0.07550617, -0.01696961, -0.05868721,  0.03636197,\n",
       "        0.06325108, -0.00398699,  0.02891234, -0.0475942 , -0.01196419,\n",
       "        0.08609958,  0.01175043, -0.02922561, -0.03252362, -0.10164388,\n",
       "        0.00836214, -0.05647131, -0.02643256,  0.02471607,  0.05208577,\n",
       "       -0.03481574, -0.03007348, -0.00566087,  0.01553415,  0.02479301,\n",
       "        0.05114043, -0.04052266, -0.00955432,  0.01687682, -0.03728522,\n",
       "        0.04570643,  0.04057559, -0.00844407, -0.03076984,  0.04126113,\n",
       "        0.01468834,  0.02578894,  0.01794329,  0.00221097, -0.05726054,\n",
       "        0.04806847,  0.00959376,  0.04423461, -0.08185552,  0.07012495,\n",
       "       -0.03240283,  0.03561619,  0.0482651 , -0.03689083,  0.05943359,\n",
       "        0.03797309,  0.00515846, -0.01418263, -0.03577099,  0.02887307,\n",
       "       -0.04024225,  0.01867092, -0.04470578,  0.07991843, -0.01770851,\n",
       "       -0.00838506,  0.01154993,  0.05081416,  0.07649112,  0.00945203,\n",
       "        0.06068432,  0.03542376, -0.00072655,  0.0115224 ,  0.11361716,\n",
       "        0.02983529,  0.0435307 , -0.05770908,  0.00849598, -0.0071168 ],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Explore the word vector for \"king\" base on our trained model\n",
    "w2v_model.wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('night', 0.9925839304924011),\n",
       " ('operator', 0.992581844329834),\n",
       " ('everything', 0.9925348162651062),\n",
       " ('price', 0.9924951195716858),\n",
       " ('ve', 0.9924671649932861),\n",
       " ('left', 0.9924629330635071),\n",
       " ('went', 0.9924255609512329),\n",
       " ('juz', 0.9924141764640808),\n",
       " ('hope', 0.9923813939094543),\n",
       " ('wk', 0.9923660159111023)]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Find the most similar words to \"king\" based on word vectors from our trained model\n",
    "w2v_model.wv.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python394jvsc74a57bd01e315a83a4e53ed53959f6fe154dabac6161a453eccd2e252f6243041d5985d2",
   "display_name": "Python 3.9.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "1e315a83a4e53ed53959f6fe154dabac6161a453eccd2e252f6243041d5985d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}